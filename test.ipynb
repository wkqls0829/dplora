{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmw/.conda/envs/adaflora/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import flwr as fl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from utils.prompter import Prompter\n",
    "from utils.data_utils import tokenize\n",
    "\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    LoraConfig,\n",
    ")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, handlers=[logging.StreamHandler()])\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from arguments import parse_args\n",
    "from data_loader import load_data\n",
    "\n",
    "datasets.utils.logging.set_verbosity_error()\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='google-bert/bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "net = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google-bert/bert-base-cased\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "tokenizer.pad_token_id = (\n",
    "    0\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "prompter = Prompter()\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False, \n",
    "    target_modules=[\"query\", \"key\", \"value\"],\n",
    "    # target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    r=16, \n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 109,225,540 || trainable%: 0.8100\n"
     ]
    }
   ],
   "source": [
    "net = get_peft_model(net, peft_config)\n",
    "net.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create a dataset\u001b[39;00m\n\u001b[1;32m     10\u001b[0m dataset_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m'\u001b[39m: instructions,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m: inputs,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m: outputs  \u001b[38;5;66;03m# Rename to 'labels' for the target variable\u001b[39;00m\n\u001b[1;32m     14\u001b[0m }\n\u001b[0;32m---> 15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_dict(dataset_dict)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "with open('./pubmed/data/training_0.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create lists for instructions, inputs, and outputs\n",
    "instructions = [item['instruction'] for item in data]\n",
    "inputs = [' '.join(item['input']) for item in data]  # Combining input fields into one string\n",
    "outputs = [item['output'] for item in data]\n",
    "\n",
    "# Create a dataset\n",
    "dataset_dict = {\n",
    "    'instruction': instructions,\n",
    "    'input': inputs,\n",
    "    'output': outputs  # Rename to 'labels' for the target variable\n",
    "}\n",
    "dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/450 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 450/450 [00:00<00:00, 780.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"instruction\"], example[\"input\"], example[\"output\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare data for Trainer\n",
    "train_dataset = tokenized_dataset.remove_columns(['instruction', 'input', \"output\"])\n",
    "train_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,   101,  2825,\n",
       "         23971,   140,  3843,  2703,  2918,  2489,  9318,  1107,  4420,  1114,\n",
       "          6703, 22869,  1116,   136,   102,  9974,  2918,  2489,  9318,  2076,\n",
       "           146,  1110,  5165,   188, 17162,  6451,  7903, 23817,   119,   138,\n",
       "          9760,  2629,  1104, 23971,   140,   113,  1112, 19248, 15421,  5190,\n",
       "           114,  1144,  1151,  2103,  2331,   119,   138, 13753,   118,  2593,\n",
       "          2025,  1108,  2011,  1106, 17459,  1157,  2629,  1107,  4420,  1114,\n",
       "          6703, 22869,  1116,   119,  1130,   170,  2702,   118,  7198,   117,\n",
       "         19916,   117,  4321,  8298,  1200,  3443,   117,  3746,  1545,  4420,\n",
       "          1114,  3565,  1559,  6703, 22869,  1116,  1127, 19729, 11117,  1106,\n",
       "          3252,  1114,  1282,  4043,  1137,  3252,  1114,  2363,   117,  2260,\n",
       "           117,  1137, 10204, 17713,  1104, 23971,   140,  3828,  1111,  5547,\n",
       "          1552,   119,  1109,  2629,  1104,  5772,   117,  1425,   117, 22869,\n",
       "          2076,   117,  1105,  2641,   118,  2272, 11344,  1113,  1103, 15299,\n",
       "          1104,  2703,  2918,  2489,  9318,  1108, 17689,   119,  2677,  2937,\n",
       "          1105, 10439,  4420,  1114,  2724,  1604, 22869,  1116,  1127,  7091,\n",
       "          2200,  1106,  3531, 23971,   140,   117,  1105, 16696,   118,  2551,\n",
       "          4420,  1114, 16696,   118,  2551, 22869,  1116,  1127,  7091,  2200,\n",
       "          1106,  3531,   170,  1282,  4043,   119,  1109, 22760,  1104,  2703,\n",
       "          2918,  2489,  9318,  1108,   123,   119,   125,   110,   113,  2022,\n",
       "          1104,  2724,  1604,   114,  1107,  1103, 23971,   140,  1372,  1105,\n",
       "          1275,   119,   122,   110,   113,  1995,  1104, 16696,   118,  2551,\n",
       "           114,  1107,  1103,  1282,  4043,  1372,   113,   185,   134,   121,\n",
       "           119,  3135,  1477,   114,   132,  1155,  1104,  1103,  4634,  4420,\n",
       "          1127,  9808,  1535,   119, 12504,  1104,  1103,  1472, 24429,  1104,\n",
       "         23971,   140,  2799,  1115,  1103, 22760,  1104,  2703,  2918,  2489,\n",
       "          9318,  1108,   125,   119,   123,   110,   113,  1300,  1104, 16696,\n",
       "           118,  1565,   114,  1107,  1103,  2363,   118, 17713,  1372,   113,\n",
       "          5236,  3187,   117,   121,   119,  3746,   132,  4573,   110,  6595,\n",
       "         14235,   117,   121,   119,  1492,  1106,   122,   119,  1765,   114,\n",
       "           117,   122,   119,   129,   110,   113,  1160,  1104, 12620,   114,\n",
       "          1107,  1103,  2260,   118, 17713,  1372,   113,  5236,  3187,   117,\n",
       "           121,   119,  1542,   132,  4573,   110,  6595, 14235,   117,   121,\n",
       "           119,  5129,  1106,   121,   119,  5581,   114,   117,  1105,   122,\n",
       "           119,   128,   110,   113,  1160,  1104, 13176,   114,  1107,  1103,\n",
       "         10204,   118, 17713,  1372,   113,  5236,  3187,   117,   121,   119,\n",
       "          1542,   132,  4573,   110,  6595, 14235,   117,   121,   119,  5129,\n",
       "          1106,   121,   119,  3453,   114,   119,  4503,  2641,   118,  2272,\n",
       "         11344, 10035,  1103,  1718,  1104,  2703,  2918,  2489,  9318,   113,\n",
       "          5236,  3187,   117,   126,   119,  2588,   132,  4573,   110,  6595,\n",
       "         14235,   117,   123,   119,  1492,  1106,  1492,   119,  3565,   114,\n",
       "           119,   102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'attention_mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,   101, 25118,  7937,   140, 13822,  1103, 22760,  1104,\n",
       "          2703,  2918,  2489,  9318,  1170,  6703, 22869,  1116,   119,   138,\n",
       "          3828, 13753,  1104,  2260, 17713,  1111,  5547,  1552,  1110,  6315,\n",
       "           119,   102])}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 658.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "with open('./pubmed/data/eval_0.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create lists for instructions, inputs, and outputs\n",
    "instructions = [item['instruction'] for item in data]\n",
    "inputs = [' '.join(item['input']) for item in data]  # Combining input fields into one string\n",
    "outputs = [item['output'] for item in data]\n",
    "\n",
    "# Create a dataset\n",
    "dataset_dict = {\n",
    "    'instruction': instructions,\n",
    "    'input': inputs,\n",
    "    'output': outputs  # Rename to 'labels' for the target variable\n",
    "}\n",
    "eval_dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "tokenized_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare data for Trainer\n",
    "eval_dataset = tokenized_dataset.remove_columns(['instruction', 'input', \"output\"])\n",
    "eval_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,   101,  2181,   172, 25669, 26218,\n",
       "         11745,  1179, 13280, 13601, 13523,  4490,  5822,  6366,  5616,  1107,\n",
       "          1103, 12645,  1104,  1603,   118,  6441, 12908,   112,   188,   184,\n",
       "          1279,  4184,  2328, 12909,  1107,  3577,   136,   102, 27688, 18290,\n",
       "          5970,  6105,   128,   120,  1406, 24754,  1158,  1144,  1151,  2103,\n",
       "          1106,  1129, 14739,  1107,  4267,  8517, 14226,  1158, 12908,   112,\n",
       "           188,   184,  1279,  4184,  2328, 12909,  1105,  3245, 11048,  1107,\n",
       "         13053, 14196, 27154,  1643, 22992,   119,  1438,   117,  1142,  1110,\n",
       "          1253,   170,  2187,  1104,  1199,  6392,   119,  1706,  4959,  1103,\n",
       "         20346,  5616,  1757,  1104,   172, 25669, 26218, 11745,  1179,   128,\n",
       "           120,  1406, 13280, 13601, 14226, 26174,  1111,  1603,   118,  6441,\n",
       "         12908,   112,   188,   184,  1279,  4184,  2328, 12909,  1107,  3577,\n",
       "           119,  1130,  4420,  1114, 12908,   112,   188,   184,  1279,  4184,\n",
       "          2328, 12909,   117, 11534,  1322,  2155, 22258,  2716,   117,  1120,\n",
       "          1655,  1160, 25128, 12685,  9985,  1127,  1678,  1121,  1198,  2071,\n",
       "          1103,  4816,  6718,  3702,  2528,  7776, 13380,  6698,   119,  1409,\n",
       "          1301,  2165,  1204,  3652,  1127,  1276,  1117,  2430,  7810,  1193,\n",
       "          1114,  2393, 13017,  2221, 24754,  1158,   117,   172, 25669, 26218,\n",
       "         11745,  1179,   128,   120,  1406, 13280, 13601,  2728, 27516,  2430,\n",
       "         16710, 24754,  1116,  1127,  1982,   119,  1130, 13053, 14196, 27154,\n",
       "          1643, 22992,  1120,  1103,  3621,  1465,  1108, 11534,  7747, 25128,\n",
       "         12685,  9985,  1678,  1121,  1439,   123,  3975,  2071,  1103,   184,\n",
       "          1279,  4184,  2328,  2758, 11305, 11048,  6698,  3090,  1107, 13053,\n",
       "         14196, 27154,  1643, 22992,   119, 12908,   112,   188,   172, 25669,\n",
       "         26218, 11745,  1179,   128,   120,  1406,  4844,  1108,  3393,  1112,\n",
       "           172, 25669, 26218, 11745,  1179,  1406,   185,  2155, 17030, 14499,\n",
       "          1107,  1178,  1103, 26558,   176,  1931,   117,  3490,  1114,   172,\n",
       "         25669, 26218, 11745,  1179,   128,   185,  2155, 17030, 14499,  1107,\n",
       "          1241,  1103, 26558,  1105,  1996, 26310,   119, 12908,   112,   188,\n",
       "           172, 25669, 26218, 11745,  1179,   128,   120,  1406,  4844,  1108,\n",
       "          4379,  1107,  1743,  1149,  1104,  3164,  2740,   113,  5581,   119,\n",
       "           129,   110,   114,  1114,  1603,   118,  6441, 12908,   112,   188,\n",
       "           184,  1279,  4184,  2328, 12909,   117,  1429,  1149,  1104,  1743,\n",
       "          2740,   113,  3614,   119,   124,   110,   114,  1114,  1107, 13053,\n",
       "         14196, 27154,  1643, 22992,  1120,  1103,  3621,  1465,   117,  1105,\n",
       "          2551,  1149,  1104,  5391,  2740,   113,  1489,   119,   129,   110,\n",
       "           114,  1114,  3245, 11048,  1107, 13053, 14196, 27154,  1643, 22992,\n",
       "           119,  1109, 15750,  1105,  2747,  1785,  1104, 12908,   112,   188,\n",
       "           172, 25669, 26218, 11745,  1179,   128,   120,  1406,  4844,  1127,\n",
       "          5581,   119,   129,  1105,  5581,   119,   126,   110,   117,  3569,\n",
       "           119,   102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'attention_mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,   101, 12908,   112,   188,   172, 25669,\n",
       "         26218, 11745,  1179,   128,   120,  1406,  4844,  1169,  1129,   170,\n",
       "          5616, 13537,  1111,  1103, 12645,  1104,  1603,   118,  6441, 12908,\n",
       "           112,   188,   184,  1279,  4184,  2328, 12909,   117,  1780,  1103,\n",
       "          6014,  3112,  1137,  6014,  4366,  2603,  1110,  2324,  1512,   110,\n",
       "           119,   102])}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(\"./pubmed/data\", train_data_name)\n",
    "eval_path = os.path.join(\"./pubmed/data\", eval_data_name)\n",
    "test_path = os.path.join(\"./pubmed/data\", test_data_name)\n",
    "\n",
    "train_data = load_dataset(\"json\", data_files=train_path)\n",
    "eval_data = load_dataset(\"json\", data_files=eval_path)\n",
    "test_data = load_dataset(\"json\", data_files=test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'ID'],\n",
       "        num_rows: 450\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 450/450 [00:00<00:00, 615.16 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 597.41 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 430.17 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data is\n",
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'ID', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 450\n",
      "})\n",
      "450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(\"./pubmed/data\", train_data_name)\n",
    "eval_path = os.path.join(\"./pubmed/data\", eval_data_name)\n",
    "test_path = os.path.join(\"./pubmed/data\", test_data_name)\n",
    "\n",
    "train_data = load_dataset(\"json\", data_files=train_path)\n",
    "eval_data = load_dataset(\"json\", data_files=eval_path)\n",
    "test_data = load_dataset(\"json\", data_files=test_path)\n",
    "\n",
    "train_data = train_data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "eval_data = eval_data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "test_data = test_data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "\n",
    "train_data.set_format(\"torch\")\n",
    "eval_data.set_format(\"torch\")\n",
    "test_data.set_format(\"torch\")\n",
    "\n",
    "\n",
    "print(\"train data is\")\n",
    "print(train_data)\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_local_trainer(net,\n",
    "                        local_train_dataset,\n",
    "                        local_eval_dataset,\n",
    "                        optim,\n",
    "                        tokenizer,\n",
    "                        local_micro_batch_size,\n",
    "                        gradient_accumulation_steps,\n",
    "                        local_num_epochs,\n",
    "                        local_learning_rate,\n",
    "                        group_by_length,\n",
    "                        warmup=0,\n",
    "                        density=None,\n",
    "                        lambd=None,\n",
    "                        reg=None):\n",
    "\n",
    "    class reg_Trainer(transformers.Trainer):\n",
    "        def compute_loss(net, inputs, return_outputs=False):\n",
    "            outputs = net(**inputs)\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "            regularizer = 0\n",
    "            count = 0\n",
    "            loss += lambd * regularizer\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def compute_metrics(pred):\n",
    "        pred_ids, labels_ids = pred\n",
    "        labels_ids = np.where(labels_ids != -100, labels_ids, tokenizer.pad_token_id)\n",
    "        pred_ids = np.where(pred_ids != -100, pred_ids, tokenizer.pad_token_id)\n",
    "        pred_ids = np.argmax(pred_ids, axis=-1)\n",
    "        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "        rouge = evaluate.load('./evaluate/metrics/rouge/rouge.py')\n",
    "        rouge_output = rouge.compute(predictions=pred_str, references=label_str, use_aggregator=True)\n",
    "        return {\n",
    "            'rouge1': round(rouge_output[\"rouge1\"], 4),\n",
    "            'rouge2': round(rouge_output[\"rouge2\"], 4),\n",
    "            'rougeL': round(rouge_output[\"rougeL\"], 4),\n",
    "            'rougeLsum': round(rouge_output[\"rougeLsum\"], 4),\n",
    "        }\n",
    "    \n",
    "    train_args = transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=local_micro_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_steps=warmup,\n",
    "        num_train_epochs=local_num_epochs,\n",
    "        learning_rate=local_learning_rate,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        logging_steps=1,\n",
    "        optim=optim,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        output_dir=\"./outputs\",\n",
    "        ddp_find_unused_parameters=False,\n",
    "        group_by_length=False,\n",
    "        dataloader_drop_last=False,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    local_trainer = transformers.Trainer(model=net,\n",
    "                                         train_dataset=local_train_dataset,\n",
    "                                         eval_dataset=local_eval_dataset,\n",
    "                                         args=train_args,\n",
    "                                         data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "                                             tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "                                         ),\n",
    "                                         compute_metrics=compute_metrics,\n",
    "                                        )\n",
    "    return local_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "local_trainer=build_local_trainer(net=net,\n",
    "                                    local_train_dataset=train_dataset,\n",
    "                                    local_eval_dataset=eval_dataset,\n",
    "                                    optim=\"adamw_torch\",\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    local_micro_batch_size=4,\n",
    "                                    gradient_accumulation_steps=32//4,\n",
    "                                    local_num_epochs=10,\n",
    "                                    local_learning_rate=1e-5,\n",
    "                                    group_by_length=False,\n",
    "                                    warmup=0,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 24.1684, 'grad_norm': 5.404697418212891, 'learning_rate': 9e-06, 'epoch': 0.5333333333333333}\n",
      "{'loss': 24.3437, 'grad_norm': 5.508152008056641, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.0666666666666667}\n",
      "{'loss': 24.2475, 'grad_norm': 5.479374408721924, 'learning_rate': 7e-06, 'epoch': 1.6}\n",
      "{'loss': 24.1153, 'grad_norm': 5.488589763641357, 'learning_rate': 6e-06, 'epoch': 2.1333333333333333}\n",
      "{'loss': 24.2509, 'grad_norm': 5.552163124084473, 'learning_rate': 5e-06, 'epoch': 2.6666666666666665}\n",
      "{'loss': 24.1805, 'grad_norm': 5.482497215270996, 'learning_rate': 4.000000000000001e-06, 'epoch': 3.2}\n",
      "{'loss': 24.1211, 'grad_norm': 5.541659832000732, 'learning_rate': 3e-06, 'epoch': 3.7333333333333334}\n",
      "{'loss': 24.0958, 'grad_norm': 5.609340190887451, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.266666666666667}\n",
      "{'loss': 24.1912, 'grad_norm': 5.588045120239258, 'learning_rate': 1.0000000000000002e-06, 'epoch': 4.8}\n",
      "{'loss': 23.909, 'grad_norm': 5.494493007659912, 'learning_rate': 0.0, 'epoch': 5.333333333333333}\n",
      "{'train_runtime': 89.7147, 'train_samples_per_second': 50.159, 'train_steps_per_second': 0.111, 'train_loss': 24.16235466003418, 'epoch': 5.333333333333333}\n",
      "trained on 1 number of dataset\n",
      "{'loss': 23.909, 'grad_norm': 5.494493007659912, 'learning_rate': 0.0, 'epoch': 5.333333333333333, 'step': 10}\n",
      "{'train_runtime': 89.7147, 'train_samples_per_second': 50.159, 'train_steps_per_second': 0.111, 'total_flos': 640862461747200.0, 'train_loss': 24.16235466003418, 'epoch': 5.333333333333333, 'step': 10}\n",
      "{'train_runtime': 89.7147, 'train_samples_per_second': 50.159, 'train_steps_per_second': 0.111, 'train_loss': 24.16235466003418, 'epoch': 5.333333333333333}\n"
     ]
    }
   ],
   "source": [
    "result = local_trainer.train()\n",
    "\n",
    "print(f\"trained on {len(train_data)} number of dataset\")\n",
    "print(local_trainer.state.log_history[-2])\n",
    "print(local_trainer.state.log_history[-1])\n",
    "print(result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, tokenizer, test_data, epoch, local_micro_batch_size):\n",
    "    test_args = transformers.TrainingArguments(\n",
    "        output_dir=\"./outputs\",\n",
    "        do_train=False,\n",
    "        do_eval=True,\n",
    "        # fp16=True,\n",
    "        per_device_eval_batch_size=local_micro_batch_size,\n",
    "        dataloader_drop_last=False,\n",
    "        eval_accumulation_steps=4,\n",
    "    )\n",
    "\n",
    "    def compute_metrics(pred):\n",
    "        pred_ids, labels_ids = pred\n",
    "        labels_ids = np.where(labels_ids != -100, labels_ids, tokenizer.pad_token_id)\n",
    "        pred_ids = np.where(pred_ids != -100, pred_ids, tokenizer.pad_token_id)\n",
    "        pred_ids = np.argmax(pred_ids, axis=-1)\n",
    "        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "        from pprint import pprint\n",
    "        print(pred_str[0])\n",
    "        print(label_str[0])\n",
    "        rouge = evaluate.load('./evaluate/metrics/rouge/rouge.py')\n",
    "        rouge_output = rouge.compute(predictions=pred_str, references=label_str, use_aggregator=True)\n",
    "        return {\n",
    "            'rouge1': round(rouge_output[\"rouge1\"], 4),\n",
    "            'rouge2': round(rouge_output[\"rouge2\"], 4),\n",
    "            'rougeL': round(rouge_output[\"rougeL\"], 4),\n",
    "            'rougeLsum': round(rouge_output[\"rougeLsum\"], 4),\n",
    "        }\n",
    "\n",
    "    # init trainer\n",
    "    tester = transformers.Trainer(\n",
    "        model=net,\n",
    "        args=test_args,\n",
    "        data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ),\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    # test_dataset = self.test_data[\"train\"].shuffle().map(self.generate_and_tokenize_prompt)\n",
    "    # test_dataset = self.local_test_dataset\n",
    "    eval_dataset = test_data\n",
    "    # test_results = tester.evaluate(test_dataset)\n",
    "    eval_results = tester.evaluate(eval_dataset)\n",
    "    # logging.info('For client ' + str( self.client_id) + ', the test result is:')\n",
    "    # logging.info(test_results)\n",
    "    print('For client ' + str(0) + ', the eval result is:')\n",
    "    print(eval_results)\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". and The The Micro micro The The Testing The Testing o o the....taining inin. Is Select Is for and, or the disease for The micro,,, c Cy Testing micro card for a am of disease disease both cancer both the o., 7 and This micro hem The meta p testing in The both both o endopha, for micro p. is The is For See.. This For o.. C What Is How What -. : :....... anti How c isost.... :. Is cytokeratin immunoreactivity useful in the diagnosis of short - segment Barrett's oesophagus in Korea?. Cytokeratin 7 / 20 staining has been reported to be helpful in diagnosing Barrett's oesophagus and gastric intestinal metaplasia. However, this is still a matter of some controversy. To determine the diagnostic usefulness of cytokeratin 7 / 20 immunostaining for short - segment Barrett's oesophagus in Korea. In patients with Barrett's oesophagus, diagnosed endoscopically, at least two biopsy specimens were taken from just below the squamocolumnar junction. If goblet cells were found histologically with alcian blue staining, cytokeratin 7 / 20 immunohistochemical stains were performed. Intestinal metaplasia at the cardia was diagnosed whenever biopsy specimens taken from within 2 cm below the oesophagogastric junction revealed intestinal metaplasia. Barrett's cytokeratin 7 / 20 pattern was defined as cytokeratin 20 positivity in only the superficial gland, combined with cytokeratin 7 positivity in both the superficial and deep glands. Barrett's cytokeratin 7 / 20 pattern was observed in 28 out of 36 cases ( 77. 8 % ) with short - segment Barrett's oesophagus, 11 out of 28 cases ( 39. 3 % ) with intestinal metaplasia at the cardia, and 9 out of 61 cases ( 14. 8 % ) with gastric intestinal metaplasia. The sensitivity and specificity of Barrett's cytokeratin 7 / 20 pattern were 77. 8 and 77. 5 %, respectively..\n",
      "Barrett's cytokeratin 7 / 20 pattern can be a useful marker for the diagnosis of short - segment Barrett's oesophagus, although the false positive or false negative rate is approximately 25 %.\n",
      "{'eval_loss': 25.9945125579834, 'eval_rouge1': 0.1418, 'eval_rouge2': 0.059, 'eval_rougeL': 0.0972, 'eval_rougeLsum': 0.097, 'eval_runtime': 5.2728, 'eval_samples_per_second': 9.483, 'eval_steps_per_second': 0.379}\n",
      "For client 0, the eval result is:\n",
      "{'eval_loss': 25.9945125579834, 'eval_rouge1': 0.1418, 'eval_rouge2': 0.059, 'eval_rougeL': 0.0972, 'eval_rougeLsum': 0.097, 'eval_runtime': 5.2728, 'eval_samples_per_second': 9.483, 'eval_steps_per_second': 0.379}\n",
      "{'eval_loss': 25.9945125579834, 'eval_rouge1': 0.1418, 'eval_rouge2': 0.059, 'eval_rougeL': 0.0972, 'eval_rougeLsum': 0.097, 'eval_runtime': 5.2728, 'eval_samples_per_second': 9.483, 'eval_steps_per_second': 0.379}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "\n",
    "eval_results = test(net, tokenizer, eval_dataset, 1, 4)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'ID', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 450\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print(train_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaflora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
